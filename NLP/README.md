* These are datasets focused on NLP models. 
* BBC, disaster and sarcasm don't contain readme files. 
* Disaster and sarcasm datasets need to train with transfer learning methods to improve accuracy. Tokenize or embedded with transfer learning may increase accuracy and decrease loss. 
* There are different NLP encoding methods. I mostly encode with tokenized and embedding layers directed inside the model. Some others can try with padding first and input to embedding layers. The main difference is input shapes and input layers. 
*  If you make it with tokenized layers, the model cannot be saved in h5 format in TensorFlow.  

For beginner, 
*  For those who want to do further study, there are many transfer learning models for data embedding and text generation. 
*  For those who want to train other datasets or search for projects, there are scam detection, recommendation, text to text translation. 