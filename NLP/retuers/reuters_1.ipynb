{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These reuters and imdb dataset are abit tricky to minpulate the data and hard to train with lstm, gru and bilateral lstm models\n",
    "I think that it is becuase the input size are too heavy and big it almost take 30 min and more ram with bilateral lstm models.\n",
    "I think I have to try with transformer model and also I need take and try with plain text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.set_seed = 42\n",
    "epoch = 10\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_features : (8982,) object \n",
      "Train_labels : (8982,) int64 \n",
      "Test features : (2246,) object \n",
      "Test_labels : (2246,) int64 \n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "(train_features,train_labels), (test_features, test_labels) = tf.keras.datasets.reuters.load_data(num_words=max_vocab_length)\n",
    "\n",
    "# Check the data shape\n",
    "print(\n",
    "    f\"Train_features : {train_features.shape} {train_features.dtype} \\n\" \n",
    "    f\"Train_labels : {train_labels.shape} {train_labels.dtype} \\n\" \n",
    "    f\"Test features : {test_features.shape} {test_features.dtype} \\n\" \n",
    "    f\"Test_labels : {test_labels.shape} {test_labels.dtype} \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[1.000e+00 3.267e+03 6.990e+02 3.434e+03 2.295e+03 5.600e+01 2.000e+00\n",
      " 7.511e+03 9.000e+00 5.600e+01 3.906e+03 1.073e+03 8.100e+01 5.000e+00\n",
      " 1.198e+03 5.700e+01 3.660e+02 7.370e+02 1.320e+02 2.000e+01 4.093e+03\n",
      " 7.000e+00 2.000e+00 4.900e+01 2.295e+03 2.000e+00 1.037e+03 3.267e+03\n",
      " 6.990e+02 3.434e+03 8.000e+00 7.000e+00 1.000e+01 2.410e+02 1.600e+01\n",
      " 8.550e+02 1.290e+02 2.310e+02 7.830e+02 5.000e+00 4.000e+00 5.870e+02\n",
      " 2.295e+03 2.000e+00 2.000e+00 7.750e+02 7.000e+00 4.800e+01 3.400e+01\n",
      " 1.910e+02 4.400e+01 3.500e+01 1.795e+03 5.050e+02 1.700e+01 1.200e+01], shape=(56,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tensor_test = tf.cast(train_features[1], dtype=tf.float32)\n",
    "\n",
    "print(tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTORIZE function\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=max_vocab_length):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return tf.cast(results, dtype= tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train features  (8982, 10000)\n",
      "test features  (2246, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize and Normalize train and test to tensors with 10k columns\n",
    "\n",
    "train_features_tf = vectorize_sequences(train_features)\n",
    "test_features_tf = vectorize_sequences(test_features)\n",
    "\n",
    "print(\"train features \", train_features_tf.shape)\n",
    "print(\"test features \", test_features_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_train_labels  (8982, 46)\n",
      "one_hot_test_labels  (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "# ONE HOT ENCODER of the labels\n",
    "\n",
    "one_hot_train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "one_hot_test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "print(\"one_hot_train_labels \", one_hot_train_labels.shape)\n",
    "print(\"one_hot_test_labels \", one_hot_test_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : <PrefetchDataset element_spec=(TensorSpec(shape=(None, 10000), dtype=tf.float32, name=None), TensorSpec(shape=(None, 46), dtype=tf.float32, name=None))> \n",
      "Test : <PrefetchDataset element_spec=(TensorSpec(shape=(None, 10000), dtype=tf.float32, name=None), TensorSpec(shape=(None, 46), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features_tf, one_hot_train_labels))\n",
    "train_dataset =  train_dataset.shuffle(8982).batch(128).cache().prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((test_features_tf,one_hot_test_labels))\n",
    "valid_dataset = valid_dataset.batch(128).cache().prefetch(tf.data.AUTOTUNE)\n",
    "print(f\"Train : {train_dataset} \\n\"\n",
    "      f\"Test : {valid_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
    "                                                  patience=5) # if val loss decreases for 3 epochs in a row, stop training\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n",
    "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
    "                                                 patience=3,\n",
    "                                                 verbose=1, # print out when learning rate goes down \n",
    "                                                 min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "embedding_layers = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=2,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length = max_length,\n",
    "                                     name=\"embedding_layers\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_Bidirectional\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10000)]           0         \n",
      "                                                                 \n",
      " embedding_layers (Embedding  (None, 10000, 2)         20000     \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              34304     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 46)                5934      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,238\n",
      "Trainable params: 60,238\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a Bidirectional RNN in TensorFlow\n",
    "inputs = layers.Input(shape=(max_vocab_length,))\n",
    "x = embedding_layers(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x) # stacking RNN layers requires return_sequences=True\n",
    "#x = layers.Bidirectional(layers.LSTM(64))(x) # bidirectional goes both ways so has double the parameters of a regular LSTM layer\n",
    "outputs = layers.Dense(46, activation=\"sigmoid\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_4_Bidirectional\")\n",
    "\n",
    "# Compile\n",
    "model_1.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Get a summary of our bidirectional model\n",
    "model_1.summary()\n",
    "\n",
    "# Fit the model (takes longer because of the bidirectional layers)\n",
    "#model_1_history = model_1.fit(train_dataset,\n",
    "#                           epochs=5,\n",
    "#                           validation_data=valid_dataset,\n",
    "#                           callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_Bidirectional\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 10000)]           0         \n",
      "                                                                 \n",
      " embedding_layers (Embedding  (None, 10000, 2)         20000     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 10000, 32)         352       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 10000, 64)         10304     \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 10000, 128)        41088     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1280000)           0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 46)                58880046  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,951,790\n",
      "Trainable params: 58,951,790\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(max_vocab_length,))\n",
    "x = embedding_layers(inputs)\n",
    "x = layers.Conv1D(32, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv1D(128, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x) # condense the output of our feature vector\n",
    "outputs = layers.Dense(46, activation=\"softmax\")(x)\n",
    "model_1= tf.keras.Model(inputs, outputs, name=\"model_4_Bidirectional\")\n",
    "\n",
    "# Compile\n",
    "model_1.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_1.summary()\n",
    "\n",
    "# start = datetime.now()\n",
    "# model_1_history = model_1.fit(train_dataset,\n",
    "#                            epochs=5,\n",
    "#                            validation_data=valid_dataset,\n",
    "#                            callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# end = datetime.now()\n",
    "# print(f\"The time taken to fit the modle is {end - start}\")\n",
    "# model_1.evaluate(valid_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hub and USE emcode layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "(train_features,train_labels), (test_features, test_labels) = tf.keras.datasets.reuters.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30979"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tf.keras.datasets.reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "len(reverse_word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokenized_sentence(sentences_array):\n",
    "    decode_sent = []\n",
    "    for values in sentences_array:\n",
    "        decoded_newswire = [' '.join([reverse_word_index.get( i - 3, '?') for i in values ]) ]\n",
    "        decode_sent.append(decoded_newswire)\n",
    "    return tf.constant(tf.squeeze(decode_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([8982]), TensorShape([2246]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_decode = decode_tokenized_sentence(train_features)\n",
    "test_decode = decode_tokenized_sentence(test_features)\n",
    "train_decode.shape, test_decode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_hub.keras_layer.KerasLayer at 0x7f53f1425c00>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[], # shape of inputs coming to our model \n",
    "                                        dtype=tf.string, # data type of inputs coming to the USE layer\n",
    "                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n",
    "                                        name=\"USE\")\n",
    "\n",
    "sentence_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_USE\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 46)                2990      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,833,646\n",
      "Trainable params: 35,822\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#BUild USE model \n",
    "# inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "# x = sentence_encoder_layer(inputs)\n",
    "# x = layers.Conv1D(32, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "# x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "# x = layers.Conv1D(128, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "# x = layers.Flatten()(x) # condense the output of our feature vector\n",
    "# outputs = layers.Dense(46, activation=\"softmax\")(x)\n",
    "# model_USE= tf.keras.Model(inputs, outputs, name=\"USE\")\n",
    "\n",
    "model_USE = tf.keras.Sequential([\n",
    "  sentence_encoder_layer, # take in sentences and then encode them into an embedding\n",
    "  layers.Dense(64, activation=\"relu\"),\n",
    "  layers.Dense(46, activation=\"sigmoid\")\n",
    "], name=\"model_USE\")\n",
    "\n",
    "# Compile\n",
    "model_USE.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_USE.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "281/281 [==============================] - 7s 24ms/step - loss: 0.6695 - accuracy: 0.8367 - val_loss: 0.8375 - val_accuracy: 0.7921 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 7s 23ms/step - loss: 0.6493 - accuracy: 0.8457 - val_loss: 0.8218 - val_accuracy: 0.7996 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 6s 23ms/step - loss: 0.6288 - accuracy: 0.8477 - val_loss: 0.8223 - val_accuracy: 0.7947 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 6s 23ms/step - loss: 0.6137 - accuracy: 0.8515 - val_loss: 0.8207 - val_accuracy: 0.8001 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 6s 23ms/step - loss: 0.5977 - accuracy: 0.8552 - val_loss: 0.8137 - val_accuracy: 0.7965 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 6s 23ms/step - loss: 0.5828 - accuracy: 0.8570 - val_loss: 0.8052 - val_accuracy: 0.8032 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 6s 23ms/step - loss: 0.5710 - accuracy: 0.8574 - val_loss: 0.7998 - val_accuracy: 0.8085 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 6s 23ms/step - loss: 0.5582 - accuracy: 0.8617 - val_loss: 0.8050 - val_accuracy: 0.8028 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 6s 23ms/step - loss: 0.5454 - accuracy: 0.8651 - val_loss: 0.8024 - val_accuracy: 0.8041 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.5338 - accuracy: 0.8677\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "281/281 [==============================] - 6s 23ms/step - loss: 0.5344 - accuracy: 0.8677 - val_loss: 0.8022 - val_accuracy: 0.8059 - lr: 0.0010\n",
      "The time taken to fit the modle is 0:01:04.611822\n",
      "71/71 [==============================] - 1s 18ms/step - loss: 0.8022 - accuracy: 0.8059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8021830916404724, 0.8058770895004272]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "model_USE_history = model_USE.fit(train_decode,\n",
    "                                  one_hot_train_labels,\n",
    "                                  epochs=10,\n",
    "                                  validation_data=[test_decode,one_hot_test_labels],\n",
    "                                  callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "end = datetime.now()\n",
    "print(f\"The time taken to fit the modle is {end - start}\")\n",
    "model_USE.evaluate(test_decode,one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 19 2022, 17:35:49) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fcd31d782b6120f2993dc67dd18a19e8000177e7f61203562e17b1bd19b8f69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
